<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta name="description" content="PRIMAL is a generative motion model that works in realtime.">
      <meta name="author" content="Yan Zhang">
      <title>PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning</title>
      <meta property="og:title" content="PRIMAL" />
      <meta property="og:description" content="PRIMAL is a generative motion model that works in realtime.">
      <meta property="og:image" content="https://yz-cnsdqz.github.io/eigenmotion/PRIMAL/images/PRIMAL-teaser.jpg" />

      <meta name="twitter:card" content="summary_large_image" />
      <meta name="twitter:title" content="PRIMAL" />
      <meta name="twitter:description" content="PRIMAL is a generative motion model that works in realtime." />
      <meta name="twitter:image" content="https://yz-cnsdqz.github.io/eigenmotion/PRIMAL/images/PRIMAL-teaser.jpg" />
      <meta name="twitter:image:alt" content="PRIMAL 2025" />
      <!-- Bootstrap core CSS -->
      <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
      <!-- Custom styles for this template -->
    <link href="css/scrolling-nav.css" rel="stylesheet">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet">

  </head>
   <body id="page-top">
      <!-- Navigation -->
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
         <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">PRIMAL</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
               <ul class="navbar-nav ml-auto">
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#about">About</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#video">Video</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#poster">Poster</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#related">Related Projects</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#pubs">Citation</a>
                  </li>
               </ul>
            </div>
         </div>
      </nav>
      <header class="bg-light text-black">
         <div class="container text-center">
            <h1>PRIMAL </h1>
            <h2>Physically Reactive and Interactive Motor Model for Avatar Learning</h2>
            <h4>ICCV, 2025</h4>
            <hr>
         </div>

         <div class="container text-center">
            <p class="lead">
               <a href="https://yz-cnsdqz.github.io" target="_blank">Yan Zhang</a><sup>1</sup>, 
               <a href="https://yfeng95.github.io" target="_blank">Yao Feng</a><sup>1,3</sup>, 
               <a href="https://is.mpg.de/person/acseke" target="_blank">Alpár Cseke</a><sup>1</sup>, 
               <a href="https://is.mpg.de/person/nsaini" target="_blank">Nitin Saini</a><sup>1</sup>, 
               <a href="https://www.linkedin.com/in/bajandas/?originalSubdomain=de" target="_blank">Nathan Bajandas</a><sup>1</sup>, 
               <a href="https://www.linkedin.com/in/nicolasheron/" target="_blank">Nicolas Heron</a><sup>1</sup>, 
               <a href="https://ps.is.mpg.de/person/black" target="_blank"> Michael J. Black</a><sup>2</sup>
            </p>
               <sup>1</sup> Meshcapade,
               <sup>2</sup> Max Planck Institute for Intelligent Systems, Tübingen, 
               <sup>3</sup> Stanford University<br>
            
            <div class="downloads">
               <br><h3>
               <a class="publink" href="https://arxiv.org/abs/2503.17544" target="_blank" style="text-decoration: none"> Paper <i class="fa fa-print"></i></a> &nbsp;
               <a class="publink" href="" target="_blank" style="text-decoration: none"> Code (coming soon) <i class="fa fa-github"></i></a> &nbsp;
               <h3>
           </div>
         </div>
         
      </header>

      <section id="about" class="about-section">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <div class="img-wide text-center">
                     <p><img class="img-fluid" alt="teaser" src="images/PRIMAL-teaser.png"></p>
                  </div>
                  <hr>
                  <p class="lead text-justify">
                     To build a motor system of the interactive avatar, it is essential to develop a generative motion model that drives the body to move through 3D space in a perpetual, realistic, controllable, and responsive manner. Although motion generation has been extensively studied, most methods do not support "<b>embodied intelligence</b>" due to their offline setting, slow speed, limited motion lengths, or unnatural movements. To overcome these limitations, we propose PRIMAL, an autoregressive diffusion model that is learned with a two-stage paradigm, inspired by recent advances in foundation models. In the pretraining stage, the model learns motion dynamics from a large number of sub-second motion segments, providing "<b>motor primitives</b>" from which more complex motions are built. In the adaptation phase, we employ a ControlNet-like adaptor to fine-tune the motor control for semantic action generation and spatial target reaching. Experiments show that physics effects emerge from our training. Given a single-frame initial state, our model not only generates unbounded, realistic, and controllable motion, but also enables the avatar to be responsive to induced impulses in real time. In addition, we can effectively and efficiently adapt our base model to few-shot personalized actions and the task of spatial control. Evaluations show that our proposed method outperforms state-of-the-art baselines. We leverage the model to create a real-time character animation system in Unreal Engine that is highly responsive and natural.
                  </p>
               </div>
            </div>
         </div>
      </section>

      <section id="video">
         <div class="container">
           <div class="row">
            <div class="col-lg-10 mx-auto">
               <h2 class="section-title-tc">Demo Videos</h2>
               <iframe width="840" height="473" src="https://www.youtube.com/embed/-GcponE1IQg?si=n_e4c4SVeFz7Jk7V" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
             </div>
           </div>
         </div>
       </section>

      <section>
        <div class="container">
          <div class="row">
            <div class="col-lg-10 mx-auto">
              <h2 class="section-title-tc">Interactive Demos</h2>
              <p class="text-justify">
                PRIMAL supports few-shot action personalization. Provided a few seconds of videos, we can fine-tune the base model with the mocapaded results. The motion realism and responsiveness are kepted, and the avatar motion style is personalized. In this demo, we retarget the generated motion to Unitree G1 character in Unreal.
              </p>
              <div class="row text-center d-flex align-items-center">
                <div class="col-md-5">
                  <div class="row no-gutters">
                    <div class="col-md-6">
                      <video autoplay loop muted playsinline width="100%">
                        <source src="videos/walk.mp4" type="video/mp4">
                      </video>
                      <p>Walk</p>
                    </div>
                    <div class="col-md-6">
                      <video autoplay loop muted playsinline width="100%">
                        <source src="videos/run.mp4" type="video/mp4">
                      </video>
                      <p>Run</p>
                    </div>
                  </div>
                </div>
                <div class="col-md-1 text-center">
                  <p style="font-size: 3rem; font-weight: bold; margin: 0;">&rarr;</p>
                </div>
                <div class="col-md-6">
                  <video autoplay loop muted playsinline width="100%">
                    <source src="videos/limped_poke.mp4" type="video/mp4">
                  </video>
                  <p>Poke while limping</p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>



      <section id="local-videos">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc">More insights on the 0.5-second motion segment.</h2>
                  <p>
                      <b>The key novelty is the formulation that generates 0.5-second motion given a single initial state.</b> This contrasts with prior work that generates a long future motion conditioned on a past motion. Its benefits include reducing overfitting, making model training easier, and making the avatar reactive to impulses and classifier-based guidance.
                  </p>
                  <p>
                     To better understand the benefits of our formulation, we compare two identical settings except the motion length, where <code>ours</code> generates 15 frames given 1 frame, and <code>baseline</code> generates 40 frames given 20 frames. We replace in-context with cross-attention to handle multi-frame conditioning. Both models are successfully overfit to a ballet sequence with 229 frames, and the ballet motion can be reproduced given the first frame(s). 
                  </p>
                  
                  <div class="row mb-4">
                     <div class="col-md-4 mb-4">
                        <video controls width="100%" poster="images/video3-thumb.png">
                           <source src="videos/balle1_the_training_seq.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;">Ballet motion for training.</p>
                     </div>
                     <div class="col-md-4 mb-4">
                        <video controls width="100%" poster="images/video4-thumb.png">
                           <source src="videos/ours_from_first.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>ours</code> given the first frame.</p>
                     </div>
                     <div class="col-md-4 mb-4">
                        <video controls width="100%" poster="images/video5-thumb.png">
                           <source src="videos/balle1_the_training_seq.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>baseline</code> given the first 20 frames.</p>
                     </div>
                  </div>
                  <p>
                     First, we generate 780 future frames given the end frame(s) of that ballet sequence, and use ASR to measure the foot-skating ratio. We find <code>ours</code> produces ballet stably, whereas <code>baseline</code> gradually fails as time progresses. 
                  </p>
                  <div class="row">
                     <div class="col-md-6 mb-4">
                        <video controls width="100%" poster="images/video1-thumb.png">
                           <source src="videos/exp1-ours.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>ours</code>, ASR=0.08. </p>
                     </div>
                     <div class="col-md-6 mb-4">
                        <video controls width="100%" poster="images/video2-thumb.png">
                           <source src="videos/exp1-baseline.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>baseline</code>, ASR=0.12.</p>
                     </div>
                  </div>

                  <p>
                     Second, we generate 156 frames, with conditions from another walking sequence. We find <code>ours</code> produces fast and natural transitions to ballet, whereas <code>baseline</code> produces severe artifacts. 
                  </p>
                  <div class="row">
                     <div class="col-md-6 mb-4">
                        <video controls width="100%" poster="images/video1-thumb.png">
                           <source src="videos/exp2-ours.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>ours</code>, ASR=0.06. </p>
                     </div>
                     <div class="col-md-6 mb-4">
                        <video controls width="100%" poster="images/video2-thumb.png">
                           <source src="videos/exp2-baseline.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>baseline</code>, ASR=0.3.</p>
                     </div>
                  </div>
                  <p>
                     <b>These results indicate our setting makes the model more generalizable w.r.t. motion length and semantics.</b>
                  </p>
               
               </div>
            </div>
         </div>
      </section>




      <section id="related">
         <div class="container">
           <div class="row">
             <div class="col-lg-10 mx-auto">
               <h2 class="section-title-tc">Related Works</h2>
               <b>PRIMAL is inspired by but not limited to the following works</b>:
               <br>
               <a href="https://tr3e.github.io/omg-page/" target="_blank" style="text-decoration: none">OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers</a>
               <br>
               <a href="https://guytevet.github.io/CLoSD-page/" target="_blank" style="text-decoration: none">CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control</a>
               <br>
               <a href="https://zkf1997.github.io/DART/" target="_blank" style="text-decoration: none">DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control</a>
               <br>
               <a href="https://aiganimation.github.io/CAMDM/" target="_blank" style="text-decoration: none">Taming Diffusion Probabilistic Models for Character Control</a>
               <br>
               <a href="https://xbpeng.github.io/projects/AMDM/index.html" target="_blank" style="text-decoration: none">Interactive Character Control with Auto-Regressive Motion Diffusion Models</a>
               <br>
               <a href="https://wandr.is.tue.mpg.de" target="_blank" style="text-decoration: none">WANDR: Intention-guided Human Motion Generation</a>
               <br>
               <a href="https://yz-cnsdqz.github.io/eigenmotion/GAMMA/" target="_blank" style="text-decoration: none">GAMMA: The Wanderings of Odysseus in 3D Scenes</a>
               <br>
               <a href="https://zkf1997.github.io/DIMOS/" target="_blank" style="text-decoration: none">DIMOS: Synthesizing Diverse Human Motions in 3D Indoor Scenes</a>
               <br>
               <a href="https://ego-gen.github.io" target="_blank" style="text-decoration: none">EgoGen: An Egocentric Synthetic Data Generator</a>
               <br>
               <a href="https://jiawei-ren.github.io/projects/insactor/index.html" target="_blank" style="text-decoration: none">InsActor: Instruction-driven Physics-based Characters</a>
               <br>

             </div>
           </div>
         </div>
       </section>
       

      <section id="pubs" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc">Citation</h2>
<pre style="display: block; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px">
   @article{zhang2025primal,
      title={PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning},
      author={Zhang, Yan and Feng, Yao and Cseke, Alp{\'a}r and Saini, Nitin and Bajandas, Nathan and Heron, Nicolas and Black, Michael J},
      journal={ICCV},
      year={2025}
  }</pre>
               </div>
            </div>
         </div>
      </section>

      
      <!-- Footer -->
      <footer class="py-5 bg-dark">
         <div class="container">
            <p class="m-0 text-center text-white">Copyright &copy; PRIMAL 2025</p>
         </div>
         <!-- /.container -->
      </footer>
      <!-- Bootstrap core JavaScript -->
      <script src="vendor/jquery/jquery.min.js"></script>
      <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
      <!-- Plugin JavaScript -->
      <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
      <!-- Custom JavaScript for this theme -->
      <script src="js/scrolling-nav.js"></script>
   </body>
</html>
