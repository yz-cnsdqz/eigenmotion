<!DOCTYPE html>
<html lang="en">
   <head>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
      <meta name="description" content="PRIMAL is a generative motion model that works in realtime.">
      <meta name="author" content="Yan Zhang">
      <title>PRIMAL: Physically Reactive and Interactive Motor Model for Avatar Learning</title>
      <meta property="og:title" content="PRIMAL" />
      <meta property="og:description" content="PRIMAL is a generative motion model that works in realtime.">
      <meta property="og:image" content="https://yz-cnsdqz.github.io/eigenmotion/PRIMAL/images/PRIMAL-teaser.jpg" />

      <meta name="twitter:card" content="summary_large_image" />
      <meta name="twitter:title" content="PRIMAL" />
      <meta name="twitter:description" content="PRIMAL is a generative motion model that works in realtime." />
      <meta name="twitter:image" content="https://yz-cnsdqz.github.io/eigenmotion/PRIMAL/images/PRIMAL-teaser.jpg" />
      <meta name="twitter:image:alt" content="PRIMAL 2025" />
      <!-- Bootstrap core CSS -->
      <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
      <!-- Custom styles for this template -->
    <link href="css/scrolling-nav.css?v=1.1" rel="stylesheet">
    <link href="css/responsive-video.css" rel="stylesheet">

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet">

  </head>
   <body id="page-top">
      <!-- Navigation -->
      <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
         <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">PRIMAL</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
               <ul class="navbar-nav ml-auto">
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#about">About</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#faq">FAQs</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#video">Video</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#poster">Poster</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#related">Related Projects</a>
                  </li>
                  <li class="nav-item">
                     <a class="nav-link js-scroll-trigger" href="#pubs">License and Citation</a>
                  </li>
               </ul>
            </div>
         </div>
      </nav>
      <header class="bg-light text-black">
         <div class="container text-center">
            <h1>PRIMAL </h1>
            <h2>Physically Reactive and Interactive Motor Model for Avatar Learning</h2>
            <h4 style="color: rgb(208, 4, 4);">ICCV 2025</h4>
            <hr>
         </div>

         <div class="container text-center">
            <p class="lead">
               <a href="https://yz-cnsdqz.github.io" target="_blank">Yan Zhang</a><sup>1</sup>, 
               <a href="https://yfeng95.github.io" target="_blank">Yao Feng</a><sup>1,3</sup>, 
               <a href="https://is.mpg.de/person/acseke" target="_blank">Alpár Cseke</a><sup>1</sup>, 
               <a href="https://is.mpg.de/person/nsaini" target="_blank">Nitin Saini</a><sup>1</sup>, 
               <a href="https://www.linkedin.com/in/bajandas/?originalSubdomain=de" target="_blank">Nathan Bajandas</a><sup>1</sup>, 
               <a href="https://www.linkedin.com/in/nicolasheron/" target="_blank">Nicolas Heron</a><sup>1</sup>, 
               <a href="https://ps.is.mpg.de/person/black" target="_blank"> Michael J. Black</a><sup>2</sup>
            </p>
               <sup>1</sup> Meshcapade,
               <sup>2</sup> Max Planck Institute for Intelligent Systems, Tübingen, 
               <sup>3</sup> Stanford University<br>
            
            <div class="downloads">
               <br><h3>
               <a class="publink" href="https://arxiv.org/abs/2503.17544" target="_blank" style="text-decoration: none"> Paper <i class="fa fa-print"></i></a> &nbsp;
               <a class="publink" href="https://github.com/yz-cnsdqz/primal-release" target="_blank" style="text-decoration: none"> Code <i class="fa fa-print"></i></a> &nbsp;
               <h3>
           </div>
         </div>
         
      </header>

      <section id="about" class="about-section">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <div class="img-wide text-center">
                     <p><img class="img-fluid" alt="teaser" src="images/PRIMAL-teaser.png"></p>
                  </div>
                  <hr>
                  <p class="lead text-justify">
                     We formulate the motor system of an interactive avatar as a generative motion model that can drive the body to move through 3D space in a perpetual, realistic, controllable, and responsive manner. Although human motion generation has been extensively studied, many existing methods lack the responsiveness and realism of real human movements. Inspired by recent advances in foundation models, we propose PRIMAL, which is learned with a two-stage paradigm. In the pretraining stage, the model learns body movements from a large number of sub-second motion segments, providing a generative foundation from which more complex motions are built. This training is fully unsupervised without annotations. Given a single-frame initial state during inference, the pretrained model not only generates unbounded, realistic, and controllable motion, but also enables the avatar to be responsive to induced impulses in real time. In the adaptation phase, we employ a novel ControlNet-like adaptor to fine-tune the base model efficiently, adapting it to new tasks such as few-shot personalized action generation and spatial target reaching. Evaluations show that our proposed method outperforms state-of-the-art baselines. We leverage the model to create a real-time character animation system in Unreal Engine that feels highly responsive and natural.
                  </p>
               </div>
            </div>
         </div>
      </section>

      <section id="faq" class="bg-light">
         <div class="container py-5">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc text-center mb-4">FAQs</h2>
                  <div class="mb-4">
                     <h5>1. What is the key novelty of PRIMAL?</h5>
                     <p class="text-justify">
                        The key novelty is the formulation. Instead of using long sequences, our model only predicts the next half-second motion based on the initial frame, and runs recursively during inference. The neural network just uses standard transformer layers.
                     </p>
                  </div>
                  <div class="mb-4">
                     <h5>2. What are potential future works?</h5>
                     <p class="text-justify">
                        There are many. For example, PRIMAL does have face and hands. It is still too slow to be used for on-device computation in games. When working on robots, additional RL-based tracking in physical simulation would be necessary. 
                     </p>
                  </div>
                  <div>
                     <h5>3. How is the poking force applied to the body without physical simulation?</h5>
                     <p class="text-justify">
                        We don't use force. Instead, we reset the velocities on the body joints to induce impulses, according to Ft=m(v2-v1) (impulse-momentum theorem). The mass is assumed to be 1 here. Note that the velocity is part of the motion representation. Our diffusion model just generalizes to unseen velocities. 
                     </p>
                  </div>
                  <div>
                     <h5>4. How is unbounded motion generated during testing?</h5>
                     <p class="text-justify">
                        The diffusion model only generates 0.5-second motion based on one frame. Afterwards, we use the generated last frame as the new condition to produce the next half-second motion, and so on.
                     </p>
                  </div>
                  <div>
                     <h5>5. How does the model keep long-term stability during runtime?</h5>
                     <p class="text-justify">
                        Unlike previous works, we neither use scheduled sampling nor VQVAE-like discretization. Our motion representation is in a continuous space. After one-day training, the stability just emerges. See 5:28 at our youtube video.
                     </p>
                  </div>
                  <div>
                     <h5>6. Since the model supports classifier-based guidance, can the avatar avoid obstacles in a 3D environment?</h5>
                     <p class="text-justify">
                        Yes probably, in particular when we use some body SDF methods like VolumetricSMPL. But we did not try it in this work.
                     </p>
                  </div>
                  <div>
                     <h5>7. Does PRIMAL support more complex text-to-motion instructions, such as “move forward for 10 steps”?</h5>
                     <p class="text-justify">
                        No, PRIMAL does not use motion history and cannot remember how many steps it has taken. There are two ways to address this issue: 1. use the motion history and increase the context window. 2. use PRIMAL and another long-term memory mechanism. Note that PRIMAL can be fine-tuned with BABEL, so it can understand and generate some motions like "move forward".
                     </p>
                  </div>
               </div>
            </div>
         </div>
      </section>

      <section id="video">
         <div class="container">
           <div class="row">
            <div class="col-lg-10 mx-auto">
               <h2 class="section-title-tc">Demo Videos</h2>
               <div class="video-container">
               <iframe width="840" height="473" src="https://www.youtube.com/embed/-GcponE1IQg?si=n_e4c4SVeFz7Jk7V" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
               </div>
               
               <hr>

               <h3 class="section-title-tc">Few-shot fine tuning and animation</h3>
               <p class="text-justify">
                PRIMAL supports few-shot action personalization. Provided a few seconds of videos, we can fine-tune the base model with the mocapaded results. The motion realism and responsiveness are kepted, and the avatar motion style is personalized. In this demo, we retarget the generated motion to Unitree G1 character in Unreal.
               </p>
               <div class="row text-center d-flex align-items-center">
                 <div class="col-md-5">
                   <div class="row no-gutters">
                     <div class="col-md-6">
                       <video autoplay loop muted playsinline width="100%">
                         <source src="videos/walk.mp4" type="video/mp4">
                       </video>
                       <p>Walk</p>
                     </div>
                     <div class="col-md-6">
                       <video autoplay loop muted playsinline width="100%">
                         <source src="videos/run.mp4" type="video/mp4">
                       </video>
                       <p>Run</p>
                     </div>
                   </div>
                 </div>
                 <div class="col-md-1 text-center">
                   <p style="font-size: 3rem; font-weight: bold; margin: 0;">&rarr;</p>
                 </div>
                 <div class="col-md-6">
                   <video autoplay loop muted playsinline width="100%">
                     <source src="videos/limped_poke.mp4" type="video/mp4">
                   </video>
                   <p>Poke while limping</p>
                 </div>
               </div>

               <hr>

               <h3 class="section-title-tc"> Text-to-motion generation in real time </h3>
               <p class="text-justify">
                    PRIMAL also supports real-time text-to-motion generation. By finetuning the base model with text annotations, we can control the avatar's actions given text prompts in the terminal. The following demo is running on Macbook Pro. To ensure real-time performance on a single Apple M3 Pro chip, we employ a set of suboptimal hyper-parameters. The model's performance is better on a more powerful machine. <t style="color: rgb(208, 4, 4);"> Note that this text-to-motion feature is not included in our ICCV'25 paper.</t>
                  
                    <!-- PRIMAL also supports real-time text-to-motion generation. By finetuning the base model with text annotations,
                    <a href="https://babel.is.tue.mpg.de" target="_blank">BABEL</a>, we can control the avatar's actions given text prompts in the terminal. The following demo is running on Macbook Pro, and the frontend is based on Rust. To ensure real-time performance on a single Apple M3 Pro chip, we employ a set of suboptimal hyper-parameters. The model's performance is better on a more powerful machine. -->
               </p>
               <div class="row text-center">
                 <div class="col-md-12 text-center">
                   <video autoplay controls width="100%">//////
                     <source src="videos/t2m.mp4" type="video/mp4">
                   </video>
                 </div>
               </div>
               <hr>
               <h3 class="section-title-tc">Advantages of the 0.5-second atomic action</h3>
                  <p>
                      <b>The key novelty is the formulation that generates 0.5-second motion given a single initial state.</b> This contrasts with prior work that generates a long future motion conditioned on a past motion. Its benefits include reducing overfitting, making model training easier, and making the avatar reactive to impulses and classifier-based guidance.
                  </p>
                  <p>
                     To better understand the benefits of our formulation, we compare two identical settings except the motion length, where <code>ours</code> generates 15 frames given 1 frame, and <code>baseline</code> generates 40 frames given 20 frames. We replace in-context with cross-attention to handle multi-frame conditioning. Both models are successfully overfit to a ballet sequence with 229 frames, and the ballet motion can be reproduced given the first frame(s). 
                  </p>
                  
                  <div class="row mb-4">
                     <div class="col-md-4 mb-4">
                        <video controls width="100%" poster="images/video3-thumb.png">
                           <source src="videos/balle1_the_training_seq.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;">Ballet motion for training.</p>
                     </div>
                     <div class="col-md-4 mb-4">
                        <video controls width="100%" poster="images/video4-thumb.png">
                           <source src="videos/ours_from_first.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>ours</code> given the first frame.</p>
                     </div>
                     <div class="col-md-4 mb-4">
                        <video controls width="100%" poster="images/video5-thumb.png">
                           <source src="videos/balle1_the_training_seq.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>baseline</code> given the first 20 frames.</p>
                     </div>
                  </div>
                  <p>
                     First, we generate 780 future frames given the end frame(s) of that ballet sequence, and use ASR to measure the foot-skating ratio. We find <code>ours</code> produces ballet stably, whereas <code>baseline</code> gradually fails as time progresses. 
                  </p>
                  <div class="row">
                     <div class="col-md-6 mb-4">
                        <video controls width="100%" poster="images/video1-thumb.png">
                           <source src="videos/exp1-ours.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>ours</code>, ASR=0.08. </p>
                     </div>
                     <div class="col-md-6 mb-4">
                        <video controls width="100%" poster="images/video2-thumb.png">
                           <source src="videos/exp1-baseline.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>baseline</code>, ASR=0.12.</p>
                     </div>
                  </div>
                  <p>
                     Second, we generate 156 frames, with conditions from another walking sequence. We find <code>ours</code> produces fast and natural transitions to ballet, whereas <code>baseline</code> produces severe artifacts. 
                  </p>
                  <div class="row">
                     <div class="col-md-6 mb-4">
                        <video controls width="100%" poster="images/video1-thumb.png">
                           <source src="videos/exp2-ours.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>ours</code>, ASR=0.06. </p>
                     </div>
                     <div class="col-md-6 mb-4">
                        <video controls width="100%" poster="images/video2-thumb.png">
                           <source src="videos/exp2-baseline.mp4.mp4" type="video/mp4">
                        </video>
                        <p class="text-center mt-2" style="font-size: 0.9em;"><code>baseline</code>, ASR=0.3.</p>
                     </div>
                  </div>
                  <p>
                     <b>These results indicate our setting makes the model more generalizable w.r.t. motion length and semantics.</b>
                  </p>
            </div>
           </div>
         </div>
      </section>

      <section id="poster" class="bg-light">
         <div class="container">
            <div class="row justify-content-center">
               <div class="col-lg-8 text-center">
                  <h2 class="section-title-tc">Poster</h2>
                  <p class="lead">Click the poster to open a high-resolution version.</p>
                  <a href="https://drive.google.com/file/d/1DlO8BTJOjPzzHUR2bvTIoOIxdSl0QcQP/view?usp=sharing" target="_blank" rel="noopener noreferrer">
                     <img src="images/poster.png" alt="PRIMAL poster" class="img-fluid">
                  </a>
               </div>
            </div>
         </div>
      </section>

      <section id="related">
         <div class="container">
           <div class="row">
             <div class="col-lg-10 mx-auto">
               <h2 class="section-title-tc">Related Works</h2>
               <b>PRIMAL is inspired by but not limited to the following works</b>:
               <br>
               <a href="https://tr3e.github.io/omg-page/" target="_blank" style="text-decoration: none">OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers</a>
               <br>
               <a href="https://guytevet.github.io/CLoSD-page/" target="_blank" style="text-decoration: none">CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control</a>
               <br>
               <a href="https://zkf1997.github.io/DART/" target="_blank" style="text-decoration: none">DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control</a>
               <br>
               <a href="https://aiganimation.github.io/CAMDM/" target="_blank" style="text-decoration: none">Taming Diffusion Probabilistic Models for Character Control</a>
               <br>
               <a href="https://xbpeng.github.io/projects/AMDM/index.html" target="_blank" style="text-decoration: none">Interactive Character Control with Auto-Regressive Motion Diffusion Models</a>
               <br>
               <a href="https://wandr.is.tue.mpg.de" target="_blank" style="text-decoration: none">WANDR: Intention-guided Human Motion Generation</a>
               <br>
               <a href="https://yz-cnsdqz.github.io/eigenmotion/GAMMA/" target="_blank" style="text-decoration: none">GAMMA: The Wanderings of Odysseus in 3D Scenes</a>
               <br>
               <a href="https://zkf1997.github.io/DIMOS/" target="_blank" style="text-decoration: none">DIMOS: Synthesizing Diverse Human Motions in 3D Indoor Scenes</a>
               <br>
               <a href="https://ego-gen.github.io" target="_blank" style="text-decoration: none">EgoGen: An Egocentric Synthetic Data Generator</a>
               <br>
               <a href="https://jiawei-ren.github.io/projects/insactor/index.html" target="_blank" style="text-decoration: none">InsActor: Instruction-driven Physics-based Characters</a>
               <br>

             </div>
           </div>
         </div>
       </section>
       

      <section id="pubs" class="">
         <div class="container">
            <div class="row">
               <div class="col-lg-10 mx-auto">
                  <h2 class="section-title-tc">License and Citation</h2>
                  <p class="text-justify">
                     This software/data/materials and the associated license is for academic research purposes.
                     The software/data/material is also available for commercial licensing through Meshcapade.com.
                     For commercial use please email sales@meshcapade.com. All rights reserved on the videos presented on this page.
                  </p>
<pre style="display: block; background-color: #f5f5f5; border: 1px solid #ccc; border-radius: 4px">
@inproceedings{primal:iccv:2025,
  author = {Zhang, Yan and Feng, Yao and Cseke, Alpár and Saini, Nitin and Bajandas, Nathan and Heron, Nicolas and Black, Michael J.},  
   title = {{PRIMAL:} Physically Reactive and Interactive Motor Model for Avatar Learning},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  month = oct,
  year = {2025}
}
  }</pre>
               </div>
            </div>
         </div>
      </section>

      
      <!-- Footer -->
      <footer class="py-5 bg-dark">
         <div class="container">
            <p class="m-0 text-center text-white">Copyright &copy; PRIMAL 2025</p>
         </div>
         <!-- /.container -->
      </footer>
      <!-- Bootstrap core JavaScript -->
      <script src="vendor/jquery/jquery.min.js"></script>
      <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
      <!-- Plugin JavaScript -->
      <script src="vendor/jquery-easing/jquery.easing.min.js"></script>
      <!-- Custom JavaScript for this theme -->
      <script src="js/scrolling-nav.js"></script>
   </body>
</html>
